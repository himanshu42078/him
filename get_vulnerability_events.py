import requests
import argparse
import pandas as pd
from datetime import datetime
import logging
import warnings
import json
import re
from functools import reduce
from requests.packages.urllib3.exceptions import InsecureRequestWarning

warnings.simplefilter('ignore', InsecureRequestWarning)

logging.basicConfig(level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler("vulnerability_fetch_cloud.log"),
              logging.StreamHandler()])

BASE_URL = "https://192.168.56.17:9200/wazuh-alerts-*/_search"
AUTH = ('admin', 'admin')
SCROLL_URL = "https://192.168.56.17:9200/_search/scroll"
SCROLL_TIME = "2m"
REQUEST_TIMEOUT = 10

FIELDS_TO_COLLECT = [
    "agent.name", "agent.ip", "timestamp", "data.vulnerability.cve",
    "data.vulnerability.severity", "data.vulnerability.package.name",
    "data.vulnerability.package.version", "data.vulnerability.status",
    "data.vulnerability.type", "data.vulnerability.published",
    "data.vulnerability.reference", "data.vulnerability.score.base",
    "data.vulnerability.rationale"
]

newline_re = re.compile(r"[\r\n]+")

def get_nested(doc, dotted_path, default=""):
    try:
        return reduce(lambda d, k: d[k] if isinstance(d, dict) else default,
                      dotted_path.split('.'), doc)
    except (KeyError, TypeError):
        return default

def sanitize_field(value):
    if isinstance(value, dict):
        value = '; '.join(f"{k}: {sanitize_field(v)}" for k, v in value.items())
    elif isinstance(value, list):
        value = ', '.join(sanitize_field(v) for v in value)
    else:
        value = str(value)
    return newline_re.sub(" ", value).strip()

def fetch_vulnerabilities_scroll(severity=None, agent_name=None, agent_id=None,
                                  scroll_size=1000, delimiter="~", output_format="csv",
                                  since=None, until=None):
    logging.debug(f"Fetching with severity={severity}, agent_name={agent_name}, agent_id={agent_id}")
    query = {
        "size": scroll_size,
        "_source": FIELDS_TO_COLLECT,
        "query": {
            "bool": {
                "must": [
                    { "match": { "rule.groups": "vulnerability-detector" } }
                ]
            }
        }
    }

    if severity:
        query["query"]["bool"]["must"].append({ "terms": { "data.vulnerability.severity": severity } })
    if agent_name:
        query["query"]["bool"]["must"].append({ "match": { "agent.name": agent_name } })
    if agent_id:
        query["query"]["bool"].setdefault("filter", []).append({ "term": { "agent.id": agent_id } })

    # Timestamp range filter
    if since or until:
        range_filter = { "range": { "@timestamp": {} } }
        if since:
            range_filter["range"]["@timestamp"]["gte"] = since
        if until:
            range_filter["range"]["@timestamp"]["lte"] = until
        query["query"]["bool"].setdefault("filter", []).append(range_filter)

    headers = {'Content-Type': 'application/json'}
    try:
        response = requests.get(f"{BASE_URL}?scroll={SCROLL_TIME}",
                                auth=AUTH, headers=headers, json=query, verify=False,
                                timeout=REQUEST_TIMEOUT)
        response.raise_for_status()
        data = response.json()
        scroll_id = data['_scroll_id']
        vulnerabilities = []
        batch_num = 1

        while True:
            documents = data.get('hits', {}).get('hits', [])
            if not documents:
                logging.info("No more documents to scroll through.")
                break

            for doc in documents:
                source = doc.get('_source', {})
                flat_document = {
                    field: sanitize_field(get_nested(source, field))
                    for field in FIELDS_TO_COLLECT
                }
                vulnerabilities.append(flat_document)

            logging.info(f"Batch {batch_num} collected with {len(documents)} entries.")
            batch_num += 1

            scroll_query = {"scroll": SCROLL_TIME, "scroll_id": scroll_id}
            scroll_response = requests.get(SCROLL_URL, auth=AUTH, headers=headers,
                                           json=scroll_query, verify=False, timeout=REQUEST_TIMEOUT)
            scroll_response.raise_for_status()
            data = scroll_response.json()
            scroll_id = data['_scroll_id']

        if output_format == "csv":
            save_to_csv(vulnerabilities, severity, agent_name, agent_id, delimiter)
        elif output_format == "json":
            save_to_json(vulnerabilities, severity, agent_name, agent_id)

    except requests.exceptions.RequestException as e:
        logging.error(f"Error fetching vulnerabilities: {e}")
        return None

def save_to_csv(vulnerabilities, severity, agent_name, agent_id, delimiter):
    df = pd.DataFrame(vulnerabilities)
    df['Severity Filtered'] = ', '.join(severity) if severity else 'All Severities'
    df['Agent Name Filtered'] = agent_name if agent_name else 'All Agents'
    df['Agent ID Filtered'] = agent_id if agent_id else 'All IDs'
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    severity_str = '_'.join(severity) if severity else 'all_severity'
    agent_name_str = agent_name if agent_name else 'all_agents'
    agent_id_str = agent_id if agent_id else 'all_ids'
    csv_file = f'wazuh_vulnerabilities_{severity_str}_{agent_name_str}_{agent_id_str}_{timestamp}.csv'
    df.to_csv(csv_file, index=False, sep=delimiter)
    logging.info(f"CSV file '{csv_file}' written with delimiter '{delimiter}'.")

def save_to_json(vulnerabilities, severity, agent_name, agent_id):
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    severity_str = '_'.join(severity) if severity else 'all_severity'
    agent_name_str = agent_name if agent_name else 'all_agents'
    agent_id_str = agent_id if agent_id else 'all_ids'
    json_file = f'wazuh_vulnerabilities_{severity_str}_{agent_name_str}_{agent_id_str}_{timestamp}.json'
    with open(json_file, 'w') as f:
        json.dump(vulnerabilities, f, indent=2)
    logging.info(f"JSON file '{json_file}' written successfully.")

def main():
    parser = argparse.ArgumentParser(description="Fetch vulnerabilities from Wazuh-Indexer using scroll API.")
    parser.add_argument('-s', '--severity', nargs='+', help="List of severity levels (e.g. Critical High)")
    parser.add_argument('-a', '--agent_name', help="Filter by agent name")
    parser.add_argument('-id', '--agent_id', help="Filter by agent ID")
    parser.add_argument('--since', help="Start time (e.g. now-24h or 2025-06-01T00:00:00Z)")
    parser.add_argument('--until', help="End time (e.g. now or 2025-06-04T23:59:59Z)")
    parser.add_argument('-z', '--size', type=int, default=1000, help="Scroll batch size (default: 1000)")
    parser.add_argument('-d', '--delimiter', default='~', help="CSV delimiter (default: ~)")
    parser.add_argument('-f', '--format', default='csv', choices=['csv', 'json'], help="Output format")
    args = parser.parse_args()

    fetch_vulnerabilities_scroll(
        severity=args.severity,
        agent_name=args.agent_name,
        agent_id=args.agent_id,
        scroll_size=args.size,
        delimiter=args.delimiter,
        output_format=args.format,
        since=args.since,
        until=args.until
    )

if __name__ == "__main__":
    main()

